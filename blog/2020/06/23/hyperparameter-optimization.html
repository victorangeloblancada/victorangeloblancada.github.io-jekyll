<!doctype html>
<html>
	<head>
		<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-5S5WB7M');</script>
<!-- End Google Tag Manager -->
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<title>Hyperparameter Optimization or The Science of Automating Model Iteration</title>
		<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Hyperparameter Optimization or The Science of Automating Model Iteration | Victor Angelo Blancada</title>
<meta name="generator" content="Jekyll v3.8.7" />
<meta property="og:title" content="Hyperparameter Optimization or The Science of Automating Model Iteration" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In this article, I will demonstrate how to use hyperopt to optimize model hyperparameters." />
<meta property="og:description" content="In this article, I will demonstrate how to use hyperopt to optimize model hyperparameters." />
<link rel="canonical" href="http://localhost:4000/blog/2020/06/23/hyperparameter-optimization.html" />
<meta property="og:url" content="http://localhost:4000/blog/2020/06/23/hyperparameter-optimization.html" />
<meta property="og:site_name" content="Victor Angelo Blancada" />
<meta property="og:image" content="http://localhost:4000/assets/images/ducks.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-06-23T00:00:00+08:00" />
<script type="application/ld+json">
{"url":"http://localhost:4000/blog/2020/06/23/hyperparameter-optimization.html","headline":"Hyperparameter Optimization or The Science of Automating Model Iteration","dateModified":"2020-06-23T00:00:00+08:00","datePublished":"2020-06-23T00:00:00+08:00","image":"http://localhost:4000/assets/images/ducks.jpg","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/blog/2020/06/23/hyperparameter-optimization.html"},"description":"In this article, I will demonstrate how to use hyperopt to optimize model hyperparameters.","@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

		<link rel="stylesheet" href="https://indestructibletype.com/fonts/Jost.css" type="text/css" charset="utf-8" />

<link rel="stylesheet" href="/assets/css/styles.css">
<link rel="stylesheet" href="/assets/css/syntax.css">

<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
	</head>
	<body>
		<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-5S5WB7M"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
		<div class="wrapper">
			<script src="/assets/js/nav.js" type="text/javascript"></script>
<div class="navbar transition" id="navbar">
	<nav>
		<a onClick="expand('navbar', 'search-input')">
			<svg width="50" height="50" aria-label="search" class="octicon octicon-search" viewBox="0 0 24 24" version="1.1" role="img"><path fill-rule="evenodd" d="M14.53 15.59a8.25 8.25 0 111.06-1.06l5.69 5.69a.75.75 0 11-1.06 1.06l-5.69-5.69zM2.5 9.25a6.75 6.75 0 1111.74 4.547.746.746 0 00-.443.442A6.75 6.75 0 012.5 9.25z"></path></svg>
			<p>Search</p>
		</a>
		
			<a href="/" >
				<svg width="50" height="50" aria-label="home" class="octicon octicon-home" viewBox="0 0 24 24" version="1.1" role="img"><path fill-rule="evenodd" d="M11.03 2.59a1.5 1.5 0 011.94 0l7.5 6.363a1.5 1.5 0 01.53 1.144V19.5a1.5 1.5 0 01-1.5 1.5h-5.75a.75.75 0 01-.75-.75V14h-2v6.25a.75.75 0 01-.75.75H4.5A1.5 1.5 0 013 19.5v-9.403c0-.44.194-.859.53-1.144l7.5-6.363zM12 3.734l-7.5 6.363V19.5h5v-6.25a.75.75 0 01.75-.75h3.5a.75.75 0 01.75.75v6.25h5v-9.403L12 3.734z"></path></svg>
				<p>Home</p>
			</a>
		
			<a href="/pages/about.html" >
				<svg width="50" height="50" aria-label="person" class="octicon octicon-person" viewBox="0 0 24 24" version="1.1" role="img"><path fill-rule="evenodd" d="M12 2.5a5.5 5.5 0 00-3.096 10.047 9.005 9.005 0 00-5.9 8.18.75.75 0 001.5.045 7.5 7.5 0 0114.993 0 .75.75 0 101.499-.044 9.005 9.005 0 00-5.9-8.181A5.5 5.5 0 0012 2.5zM8 8a4 4 0 118 0 4 4 0 01-8 0z"></path></svg>
				<p>About</p>
			</a>
		
			<a href="/blog.html" >
				<svg width="50" height="50" aria-label="book" class="octicon octicon-book" viewBox="0 0 24 24" version="1.1" role="img"><path fill-rule="evenodd" d="M0 3.75A.75.75 0 01.75 3h7.497c1.566 0 2.945.8 3.751 2.014A4.496 4.496 0 0115.75 3h7.5a.75.75 0 01.75.75v15.063a.75.75 0 01-.755.75l-7.682-.052a3 3 0 00-2.142.878l-.89.891a.75.75 0 01-1.061 0l-.902-.901a3 3 0 00-2.121-.879H.75a.75.75 0 01-.75-.75v-15zm11.247 3.747a3 3 0 00-3-2.997H1.5V18h6.947a4.5 4.5 0 012.803.98l-.003-11.483zm1.503 11.485V7.5a3 3 0 013-3h6.75v13.558l-6.927-.047a4.5 4.5 0 00-2.823.971z"></path></svg>
				<p>Blog</p>
			</a>
		
			<a href="/pages/resume-downloads.html" >
				<svg width="50" height="50" aria-label="file" class="octicon octicon-file" viewBox="0 0 24 24" version="1.1" role="img"><path fill-rule="evenodd" d="M5 2.5a.5.5 0 00-.5.5v18a.5.5 0 00.5.5h14a.5.5 0 00.5-.5V8.5h-4a2 2 0 01-2-2v-4H5zm10 0v4a.5.5 0 00.5.5h4a.5.5 0 00-.146-.336l-4.018-4.018A.5.5 0 0015 2.5zM3 3a2 2 0 012-2h9.982a2 2 0 011.414.586l4.018 4.018A2 2 0 0121 7.018V21a2 2 0 01-2 2H5a2 2 0 01-2-2V3z"></path></svg>
				<p>Résumé</p>
			</a>
		
	</nav>

	<!-- Html Elements for Search -->
	<div id="search-container" class="search-container">
		<center>
			<input type="text" id="search-input" placeholder="Search this site" style="width: 100%;">
			<ol id="results-container"></ol>
		</center>
	</div>

	<!-- Script pointing to search.js -->
	<script src="/assets/js/search.js" type="text/javascript"></script>

	<!-- Configuration -->
	<script>
	SimpleJekyllSearch({
	  searchInput: document.getElementById('search-input'),
	  resultsContainer: document.getElementById('results-container'),
	  json: '/search.json'
	})
	</script>
</div>

</div>

			<div class="hero-img"  style="background-image: linear-gradient(rgba(0,0,0,.3), rgba(0,0,0,.3)), url( /assets/images/ducks.jpg );"  >
				<div class="center-content">
					<span class="on-bg">
						<h1> Hyperparameter Optimization or The Science of Automating Model Iteration </h1>
						<!--
						<p>23 Jun 2020</p>
						-->
						<a href="#start">
							<div style="-webkit-animation-name: pulse; -webkit-animation-duration: 2s; animation-name: pulse;animation-duration: 2s; animation-direction: alternate;">
								
								<p>June 23, 2020</p>
								
								<p>scroll for more</p>
								<svg width="50" height="50" fill="white" aria-label="down" class="octicon octicon-arrow-down" viewBox="0 0 24 24" version="1.1" role="img"><path fill-rule="evenodd" d="M4.97 13.22a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 10-1.06-1.06l-4.97 4.97V3.75a.75.75 0 00-1.5 0v14.44l-4.97-4.97a.75.75 0 00-1.06 0z"></path></svg>
							</div>
						</a>
						
						<div>
							

<style>
#share-buttons {display: inline-block; vertical-align: middle; }
#share-buttons:after {content: ""; display: block; clear: both;}
#share-buttons > div {
    position: relative;
    text-align: left; 
    height: 36px; 
    width: 32px; 
    float: left; 
    text-align: center;
}
#share-buttons > div > svg {height: 16px; fill: #FFF; margin-top: 10px;}
#share-buttons > div:hover {cursor: pointer;}
#share-buttons > div.facebook:hover > svg {fill: #3B5998;}
#share-buttons > div.twitter:hover > svg {fill: #55ACEE;}
#share-buttons > div.linkedin:hover > svg {fill: #0077b5;}
#share-buttons > div.pinterest:hover > svg {fill: #CB2027;}
#share-buttons > div.gplus:hover > svg {fill: #dd4b39;}
#share-buttons > div.mail:hover > svg {fill: #7D7D7D;}
#share-buttons > div.instagram:hover > svg {fill: #C73B92;}
#share-buttons > div.facebook > svg {height: 18px; margin-top: 9px;}
#share-buttons > div.twitter > svg {height: 20px; margin-top: 8px;}
#share-buttons > div.linkedin > svg {height: 19px; margin-top: 7px;}
#share-buttons > div.pinterest > svg {height: 20px; margin-top: 9px;}
#share-buttons > div.gplus > svg {height: 17px; margin-top: 9px; position: relative; left: 1px;}
#share-buttons > div.mail > svg {height: 14px; margin-top: 11px;}
</style>

<span>Share on: </span><div id="share-buttons">

    <div class="linkedin" title="Share this on Linkedin" onclick="window.open('https://www.linkedin.com/shareArticle?mini=true&url=http://localhost:4000/blog/2020/06/23/hyperparameter-optimization.html&title=&summary=&source=');"><svg viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M477 625v991h-330v-991h330zm21-306q1 73-50.5 122t-135.5 49h-2q-82 0-132-49t-50-122q0-74 51.5-122.5t134.5-48.5 133 48.5 51 122.5zm1166 729v568h-329v-530q0-105-40.5-164.5t-126.5-59.5q-63 0-105.5 34.5t-63.5 85.5q-11 30-11 81v553h-329q2-399 2-647t-1-296l-1-48h329v144h-2q20-32 41-56t56.5-52 87-43.5 114.5-15.5q171 0 275 113.5t104 332.5z"/></svg></div>

    <div class="twitter" title="Share this on Twitter" onclick="window.open('http://twitter.com/home?status=http://localhost:4000/blog/2020/06/23/hyperparameter-optimization.html');"><svg viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M1684 408q-67 98-162 167 1 14 1 42 0 130-38 259.5t-115.5 248.5-184.5 210.5-258 146-323 54.5q-271 0-496-145 35 4 78 4 225 0 401-138-105-2-188-64.5t-114-159.5q33 5 61 5 43 0 85-11-112-23-185.5-111.5t-73.5-205.5v-4q68 38 146 41-66-44-105-115t-39-154q0-88 44-163 121 149 294.5 238.5t371.5 99.5q-8-38-8-74 0-134 94.5-228.5t228.5-94.5q140 0 236 102 109-21 205-78-37 115-142 178 93-10 186-50z"/></svg></div>
    
    <div class="facebook" title="Share this on Facebook" onclick="window.open('http://www.facebook.com/share.php?u=http://localhost:4000/blog/2020/06/23/hyperparameter-optimization.html');"><svg viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M1343 12v264h-157q-86 0-116 36t-30 108v189h293l-39 296h-254v759h-306v-759h-255v-296h255v-218q0-186 104-288.5t277-102.5q147 0 228 12z"/></svg></div>


    <!--<div class="pinterest" title="Share this on Pinterest" onclick="window.open('https://pinterest.com/pin/create/button/?url=&media=http://localhost:4000/assets/images/ducks.jpg&description=');"><svg viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M256 597q0-108 37.5-203.5t103.5-166.5 152-123 185-78 202-26q158 0 294 66.5t221 193.5 85 287q0 96-19 188t-60 177-100 149.5-145 103-189 38.5q-68 0-135-32t-96-88q-10 39-28 112.5t-23.5 95-20.5 71-26 71-32 62.5-46 77.5-62 86.5l-14 5-9-10q-15-157-15-188 0-92 21.5-206.5t66.5-287.5 52-203q-32-65-32-169 0-83 52-156t132-73q61 0 95 40.5t34 102.5q0 66-44 191t-44 187q0 63 45 104.5t109 41.5q55 0 102-25t78.5-68 56-95 38-110.5 20-111 6.5-99.5q0-173-109.5-269.5t-285.5-96.5q-200 0-334 129.5t-134 328.5q0 44 12.5 85t27 65 27 45.5 12.5 30.5q0 28-15 73t-37 45q-2 0-17-3-51-15-90.5-56t-61-94.5-32.5-108-11-106.5z"/></svg></div>-->

    <!--<div class="gplus" title="Share this on Google Plus" onclick="window.open('https://plus.google.com/share?url=http://localhost:4000/blog/2020/06/23/hyperparameter-optimization.html');"><svg viewBox="0 0 2304 1792" xmlns="http://www.w3.org/2000/svg"><path d="M1437 913q0 208-87 370.5t-248 254-369 91.5q-149 0-285-58t-234-156-156-234-58-285 58-285 156-234 234-156 285-58q286 0 491 192l-199 191q-117-113-292-113-123 0-227.5 62t-165.5 168.5-61 232.5 61 232.5 165.5 168.5 227.5 62q83 0 152.5-23t114.5-57.5 78.5-78.5 49-83 21.5-74h-416v-252h692q12 63 12 122zm867-122v210h-209v209h-210v-209h-209v-210h209v-209h210v209h209z"/></svg></div>-->

    <div class="mail" title="Share this through Email" onclick="window.open('mailto:?&body=http://localhost:4000/blog/2020/06/23/hyperparameter-optimization.html');"><svg viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M1792 710v794q0 66-47 113t-113 47h-1472q-66 0-113-47t-47-113v-794q44 49 101 87 362 246 497 345 57 42 92.5 65.5t94.5 48 110 24.5h2q51 0 110-24.5t94.5-48 92.5-65.5q170-123 498-345 57-39 100-87zm0-294q0 79-49 151t-122 123q-376 261-468 325-10 7-42.5 30.5t-54 38-52 32.5-57.5 27-50 9h-2q-23 0-50-9t-57.5-27-52-32.5-54-38-42.5-30.5q-91-64-262-182.5t-205-142.5q-62-42-117-115.5t-55-136.5q0-78 41.5-130t118.5-52h1472q65 0 112.5 47t47.5 113z"/></svg></div>

</div>
						</div>
						
					</span>
				</div>
			</div>
			<div id="hero-img-spacer" style="height: 100vh;">
			</div>
			<div id="start">
			</div>
			<div class="content">
				
				<h1>Hyperparameter Optimization or The Science of Automating Model Iteration</h1>
				
				<!--
				<h1> Hyperparameter Optimization or The Science of Automating Model Iteration </h1>
				
				<p style="font-style:italic">June 23, 2020</p>
				
				-->
				<div class="article  dropcap ">
					<p>Building a predictive model is a time-consuming task: it is not uncommon to for a data scientist to spend several sessions tweaking a predictive model to improve its accuracy. While most modern machine learning libraries only require the user to call a function to fit the model on training data, most models have settings that determine how it learns patterns from data - these settings are called hyperparameters. An example of a hyperparameter might be the maximum depth of decision tree model. The task of iterating across different variants of a model is formally called hyperparameter optimization.</p>

<p>There have been many attempts to automate hyperparameter optimization:</p>

<ol>
  <li>The <strong>grid search</strong> method iterates across a user defined set of model hyperparameters (called the hyperparameter space) to get the best combination of hyperparameters. Because of the exhaustive nature of grid searches, they are sure to find the best hyperparameters within the space but they are also computationally expensive.</li>
  <li>The <strong>random search</strong> method randomly searches the hyperparameter space for a set number of iterations and returns the best combination of hyperparameters out of the randomly selected subset of tested models. This allows the data scientist to save time and compute power since not all possible combinations are tested but this also means that the modeler cannot be certain that the selected model hyperparameters are the best within the search space.</li>
  <li>“Intelligent” search methods such as <strong>Bayesian optimization</strong> similarly do not test the entire hyperparameter space but improve upon random search by intelligently adjusting the hyperparameters in the direction of improving model accuracy based on the gradient of the accuracy metric.</li>
</ol>

<p>Both <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV">grid search</a> and <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV">random search</a> have implementations in <a href="https://scikit-learn.org/stable/modules/grid_search.html">scikit-learn</a> but I will focus on how to perform intelligent hyperparameter optimization using the <a href="https://github.com/hyperopt/hyperopt">Hyperopt</a> library.</p>

<h2 id="hyperparameter-optimization-using-hyperopt">Hyperparameter Optimization Using Hyperopt</h2>

<p>Recall that hyperparameter optimization requires using an automated algorithm to search a user-defined hyperparameter space for best combination of hyperparameters based on model accuracy metric.</p>

<p>The typical structure of a modeling script that leverages Hyperopt for hyperparameter optimization is as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">hyperopt</span> <span class="kn">import</span> <span class="n">fmin</span><span class="p">,</span> <span class="n">tpe</span><span class="p">,</span> <span class="n">hp</span>


<span class="c1"># Define the hyperparameter space
</span><span class="n">space</span> <span class="o">=</span> <span class="p">[</span>
         <span class="n">hp</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="s">'a1'</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="c1"># Uniformly distributed
</span>         <span class="n">hp</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="s">'a2'</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="c1"># Normally distributed
</span>         <span class="n">hp</span><span class="p">.</span><span class="n">quniform</span><span class="p">(</span><span class="s">'a3'</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="c1"># Uniformly distributed with discrete intervals
</span>    	 <span class="n">hp</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="s">'a4'</span><span class="p">,</span> <span class="p">[</span><span class="s">'A'</span><span class="p">,</span> <span class="s">'B'</span><span class="p">,</span> <span class="s">'C'</span><span class="p">])</span> <span class="c1"># Discrete choices
</span><span class="p">]</span>

<span class="c1"># Define the objective function
</span><span class="k">def</span> <span class="nf">function</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
    <span class="n">a1</span><span class="p">,</span> <span class="n">a2</span><span class="p">,</span> <span class="n">a3</span><span class="p">,</span> <span class="n">a4</span> <span class="o">=</span> <span class="n">args</span>
    <span class="c1"># Data transformations
</span>    <span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">.</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">**</span><span class="n">a1</span><span class="p">)</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">.</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">**</span><span class="n">a1</span><span class="p">)</span>
    <span class="c1"># Fit a model
</span>    <span class="n">reg</span> <span class="o">=</span> <span class="n">model_class</span><span class="p">(</span><span class="n">a2</span><span class="p">,</span> <span class="n">a3</span><span class="p">,</span> <span class="n">a4</span><span class="p">)</span>
    <span class="n">reg</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
    <span class="n">Y_pred</span> <span class="o">=</span> <span class="n">reg</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">Y_pred</span><span class="p">)</span>

<span class="c1"># Getting the best hyperparameters and transformations using hyperopt
</span><span class="n">best</span> <span class="o">=</span> <span class="n">fmin</span><span class="p">(</span><span class="n">function</span><span class="p">,</span><span class="n">space</span><span class="p">,</span><span class="n">algo</span><span class="o">=</span><span class="n">tpe</span><span class="p">.</span><span class="n">suggest</span><span class="p">,</span><span class="n">max_evals</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="c1"># Display the best combination of hyperparameters and transformations
</span><span class="k">print</span><span class="p">(</span><span class="n">best</span><span class="p">)</span>

<span class="c1"># Fit the final model using the hyperparameter optimization results
</span><span class="n">a1</span> <span class="o">=</span> <span class="n">best</span><span class="p">[</span><span class="s">'a1'</span><span class="p">]</span>
<span class="n">a2</span> <span class="o">=</span> <span class="n">best</span><span class="p">[</span><span class="s">'a2'</span><span class="p">]</span>
<span class="n">a3</span> <span class="o">=</span> <span class="n">best</span><span class="p">[</span><span class="s">'a3'</span><span class="p">]</span>
<span class="n">a4</span> <span class="o">=</span> <span class="n">best</span><span class="p">[</span><span class="s">'a4'</span><span class="p">]</span>
<span class="c1"># Data transformations
</span><span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">.</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">**</span><span class="n">a1</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">.</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">**</span><span class="n">a1</span><span class="p">)</span>
<span class="c1"># Fit a model
</span><span class="n">reg</span> <span class="o">=</span> <span class="n">model_class</span><span class="p">(</span><span class="n">a2</span><span class="p">,</span> <span class="n">a3</span><span class="p">,</span> <span class="n">a4</span><span class="p">)</span>
<span class="n">reg</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
<span class="n">Y_pred</span> <span class="o">=</span> <span class="n">reg</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div></div>

<p>Let’s go through that code section by section.</p>

<h2 id="defining-the-hyperparameter-space">Defining the Hyperparameter Space</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Define the hyperparameter space
</span><span class="n">space</span> <span class="o">=</span> <span class="p">[</span>
         <span class="n">hp</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="s">'a1'</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="c1"># Uniformly distributed
</span>         <span class="n">hp</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="s">'a2'</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="c1"># Normally distributed
</span>         <span class="n">hp</span><span class="p">.</span><span class="n">quniform</span><span class="p">(</span><span class="s">'a3'</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="c1"># Uniformly distributed with discrete intervals
</span>    	 <span class="n">hp</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="s">'a4'</span><span class="p">,</span> <span class="p">[</span><span class="s">'A'</span><span class="p">,</span> <span class="s">'B'</span><span class="p">,</span> <span class="s">'C'</span><span class="p">])</span> <span class="c1"># Discrete choices
</span><span class="p">]</span>
</code></pre></div></div>

<p>Hyperopt provides the <strong>hp</strong> class to represent hyperparameter distributions. Some of these distributions might be:</p>

<ul>
  <li>hp.uniform - a uniformly distributed hyperparameter</li>
  <li>hp.normal - a normally distributed hyperparameter</li>
  <li>hp.quniform - a uniformly distributed hyperparameter with discrete intervals</li>
  <li>hp.choice - a hyperparameter randomly selected from a list of choices</li>
</ul>

<p>Hyperopt expects the hyperparameter space to be a list of hp objects. The code block above defines a hyperparameter space of four hyperparameters defined by the distribution of values to be tested.</p>

<p>Note that because the hp.choice class defines a hyperparameter that randomly selects from a list of choices, it is possible to nest the hyperparameter space within an hp.choice object. This is useful when you want to test multiple model classes which take in different kinds of hyperparameters. This is illustrated in the code block below.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Define a nested hyperparameter space
</span><span class="n">space</span> <span class="o">=</span> <span class="n">hp</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="s">'space choice'</span><span class="p">,[</span>
    <span class="p">[</span><span class="n">hp</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="s">'a1'</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
     <span class="n">hp</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="s">'a2'</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">)],</span>
    <span class="p">[</span><span class="n">hp</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="s">'a1'</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
     <span class="n">hp</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="s">'a2'</span><span class="p">,</span> <span class="p">[</span><span class="s">'mae'</span><span class="p">,</span> <span class="s">'mse'</span><span class="p">])]</span>
<span class="p">])</span>
</code></pre></div></div>

<h2 id="defining-the-objective-function">Defining the Objective Function</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Define the objective function
</span><span class="k">def</span> <span class="nf">function</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
    <span class="n">a1</span><span class="p">,</span> <span class="n">a2</span><span class="p">,</span> <span class="n">a3</span><span class="p">,</span> <span class="n">a4</span> <span class="o">=</span> <span class="n">args</span>
    <span class="c1"># Data transformations
</span>    <span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">.</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">**</span><span class="n">a1</span><span class="p">)</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">.</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">**</span><span class="n">a1</span><span class="p">)</span>
    <span class="c1"># Fit a model
</span>    <span class="n">reg</span> <span class="o">=</span> <span class="n">model_class</span><span class="p">(</span><span class="n">a2</span><span class="p">,</span> <span class="n">a3</span><span class="p">,</span> <span class="n">a4</span><span class="p">)</span>
    <span class="n">reg</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
    <span class="n">Y_pred</span> <span class="o">=</span> <span class="n">reg</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">Y_pred</span><span class="p">)</span>
</code></pre></div></div>

<p>Since hyperparameter optimization requires a model accuracy metric, a function must be defined that will return this metric, as in the code block above.</p>

<p>Note that Hyperopt expects this to be a loss metric such as the mean squared error and will seek minimize this. If you instead using an accuracy metric that is better if it is higher (e.g. accuracy or R2), you may add a negative sign so that Hyperopt will seek to minimize its additive inverse.</p>

<p>The objective function takes in the selected hyperparameters from the distributions previously defined in the hyperparameter space as arguments and attempts to fit a model using these hyperparameters. Note that in this example, parameters for data variable transformations are also considered as hyperparameters (i.e. a1 controls the power transformation). The function then returns the model accuracy or loss metric for evaluation.</p>

<h2 id="automating-model-iteration">Automating Model Iteration</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Getting the best hyperparameters and transformations using hyperopt
</span><span class="n">best</span> <span class="o">=</span> <span class="n">fmin</span><span class="p">(</span><span class="n">function</span><span class="p">,</span><span class="n">space</span><span class="p">,</span><span class="n">algo</span><span class="o">=</span><span class="n">tpe</span><span class="p">.</span><span class="n">suggest</span><span class="p">,</span><span class="n">max_evals</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</code></pre></div></div>

<p>The code block above calls Hyperopt’s <strong>fmin</strong> function to minimize the loss metric we had defined in the objective function earlier (function) by testing various combinations of hyperparameters from the hyperparameter space (space). The algo argument is set to tpe.suggest to tell Hyperopt to automatically suggest the next set of hyperparameters based on the gradient of previously tested model losses. The max_evals argument is set to 1,000 to tell Hyperopt to test 1,000 combinations of hyperparameters before stopping.</p>

<p>The fmin function returns the best set of hyperparameters. We will store these hyperparameters in a variable to call later when we finalize the model.</p>

<h2 id="finalizing-the-model">Finalizing the Model</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Fit the final model using the hyperparameter optimization results
</span><span class="n">a1</span> <span class="o">=</span> <span class="n">best</span><span class="p">[</span><span class="s">'a1'</span><span class="p">]</span>
<span class="n">a2</span> <span class="o">=</span> <span class="n">best</span><span class="p">[</span><span class="s">'a2'</span><span class="p">]</span>
<span class="n">a3</span> <span class="o">=</span> <span class="n">best</span><span class="p">[</span><span class="s">'a3'</span><span class="p">]</span>
<span class="n">a4</span> <span class="o">=</span> <span class="n">best</span><span class="p">[</span><span class="s">'a4'</span><span class="p">]</span>
<span class="c1"># Data transformations
</span><span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">.</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">**</span><span class="n">a1</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">.</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">**</span><span class="n">a1</span><span class="p">)</span>
<span class="c1"># Fit a model
</span><span class="n">reg</span> <span class="o">=</span> <span class="n">model_class</span><span class="p">(</span><span class="n">a2</span><span class="p">,</span> <span class="n">a3</span><span class="p">,</span> <span class="n">a4</span><span class="p">)</span>
<span class="n">reg</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
<span class="n">Y_pred</span> <span class="o">=</span> <span class="n">reg</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div></div>

<p>Lastly, the final model is fit based on best hyperparameters saved from the results of the fmin function earlier.</p>

				</div>
				
				<div class="cta-box">
					

<style>
#share-buttons {display: inline-block; vertical-align: middle; }
#share-buttons:after {content: ""; display: block; clear: both;}
#share-buttons > div {
    position: relative;
    text-align: left; 
    height: 36px; 
    width: 32px; 
    float: left; 
    text-align: center;
}
#share-buttons > div > svg {height: 16px; fill: #FFF; margin-top: 10px;}
#share-buttons > div:hover {cursor: pointer;}
#share-buttons > div.facebook:hover > svg {fill: #3B5998;}
#share-buttons > div.twitter:hover > svg {fill: #55ACEE;}
#share-buttons > div.linkedin:hover > svg {fill: #0077b5;}
#share-buttons > div.pinterest:hover > svg {fill: #CB2027;}
#share-buttons > div.gplus:hover > svg {fill: #dd4b39;}
#share-buttons > div.mail:hover > svg {fill: #7D7D7D;}
#share-buttons > div.instagram:hover > svg {fill: #C73B92;}
#share-buttons > div.facebook > svg {height: 18px; margin-top: 9px;}
#share-buttons > div.twitter > svg {height: 20px; margin-top: 8px;}
#share-buttons > div.linkedin > svg {height: 19px; margin-top: 7px;}
#share-buttons > div.pinterest > svg {height: 20px; margin-top: 9px;}
#share-buttons > div.gplus > svg {height: 17px; margin-top: 9px; position: relative; left: 1px;}
#share-buttons > div.mail > svg {height: 14px; margin-top: 11px;}
</style>

<span>Share on: </span><div id="share-buttons">

    <div class="linkedin" title="Share this on Linkedin" onclick="window.open('https://www.linkedin.com/shareArticle?mini=true&url=http://localhost:4000/blog/2020/06/23/hyperparameter-optimization.html&title=&summary=&source=');"><svg viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M477 625v991h-330v-991h330zm21-306q1 73-50.5 122t-135.5 49h-2q-82 0-132-49t-50-122q0-74 51.5-122.5t134.5-48.5 133 48.5 51 122.5zm1166 729v568h-329v-530q0-105-40.5-164.5t-126.5-59.5q-63 0-105.5 34.5t-63.5 85.5q-11 30-11 81v553h-329q2-399 2-647t-1-296l-1-48h329v144h-2q20-32 41-56t56.5-52 87-43.5 114.5-15.5q171 0 275 113.5t104 332.5z"/></svg></div>

    <div class="twitter" title="Share this on Twitter" onclick="window.open('http://twitter.com/home?status=http://localhost:4000/blog/2020/06/23/hyperparameter-optimization.html');"><svg viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M1684 408q-67 98-162 167 1 14 1 42 0 130-38 259.5t-115.5 248.5-184.5 210.5-258 146-323 54.5q-271 0-496-145 35 4 78 4 225 0 401-138-105-2-188-64.5t-114-159.5q33 5 61 5 43 0 85-11-112-23-185.5-111.5t-73.5-205.5v-4q68 38 146 41-66-44-105-115t-39-154q0-88 44-163 121 149 294.5 238.5t371.5 99.5q-8-38-8-74 0-134 94.5-228.5t228.5-94.5q140 0 236 102 109-21 205-78-37 115-142 178 93-10 186-50z"/></svg></div>
    
    <div class="facebook" title="Share this on Facebook" onclick="window.open('http://www.facebook.com/share.php?u=http://localhost:4000/blog/2020/06/23/hyperparameter-optimization.html');"><svg viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M1343 12v264h-157q-86 0-116 36t-30 108v189h293l-39 296h-254v759h-306v-759h-255v-296h255v-218q0-186 104-288.5t277-102.5q147 0 228 12z"/></svg></div>


    <!--<div class="pinterest" title="Share this on Pinterest" onclick="window.open('https://pinterest.com/pin/create/button/?url=&media=http://localhost:4000/assets/images/ducks.jpg&description=');"><svg viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M256 597q0-108 37.5-203.5t103.5-166.5 152-123 185-78 202-26q158 0 294 66.5t221 193.5 85 287q0 96-19 188t-60 177-100 149.5-145 103-189 38.5q-68 0-135-32t-96-88q-10 39-28 112.5t-23.5 95-20.5 71-26 71-32 62.5-46 77.5-62 86.5l-14 5-9-10q-15-157-15-188 0-92 21.5-206.5t66.5-287.5 52-203q-32-65-32-169 0-83 52-156t132-73q61 0 95 40.5t34 102.5q0 66-44 191t-44 187q0 63 45 104.5t109 41.5q55 0 102-25t78.5-68 56-95 38-110.5 20-111 6.5-99.5q0-173-109.5-269.5t-285.5-96.5q-200 0-334 129.5t-134 328.5q0 44 12.5 85t27 65 27 45.5 12.5 30.5q0 28-15 73t-37 45q-2 0-17-3-51-15-90.5-56t-61-94.5-32.5-108-11-106.5z"/></svg></div>-->

    <!--<div class="gplus" title="Share this on Google Plus" onclick="window.open('https://plus.google.com/share?url=http://localhost:4000/blog/2020/06/23/hyperparameter-optimization.html');"><svg viewBox="0 0 2304 1792" xmlns="http://www.w3.org/2000/svg"><path d="M1437 913q0 208-87 370.5t-248 254-369 91.5q-149 0-285-58t-234-156-156-234-58-285 58-285 156-234 234-156 285-58q286 0 491 192l-199 191q-117-113-292-113-123 0-227.5 62t-165.5 168.5-61 232.5 61 232.5 165.5 168.5 227.5 62q83 0 152.5-23t114.5-57.5 78.5-78.5 49-83 21.5-74h-416v-252h692q12 63 12 122zm867-122v210h-209v209h-210v-209h-209v-210h209v-209h210v209h209z"/></svg></div>-->

    <div class="mail" title="Share this through Email" onclick="window.open('mailto:?&body=http://localhost:4000/blog/2020/06/23/hyperparameter-optimization.html');"><svg viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M1792 710v794q0 66-47 113t-113 47h-1472q-66 0-113-47t-47-113v-794q44 49 101 87 362 246 497 345 57 42 92.5 65.5t94.5 48 110 24.5h2q51 0 110-24.5t94.5-48 92.5-65.5q170-123 498-345 57-39 100-87zm0-294q0 79-49 151t-122 123q-376 261-468 325-10 7-42.5 30.5t-54 38-52 32.5-57.5 27-50 9h-2q-23 0-50-9t-57.5-27-52-32.5-54-38-42.5-30.5q-91-64-262-182.5t-205-142.5q-62-42-117-115.5t-55-136.5q0-78 41.5-130t118.5-52h1472q65 0 112.5 47t47.5 113z"/></svg></div>

</div>
				</div>
				
			</div>
			
			<center>
				<h2>Read more posts about...</h2>
				<ul class="tags">
					
						<a href='/blog/tag/python'><li> python </li></a>
					
						<a href='/blog/tag/hyperopt'><li> hyperopt </li></a>
					
						<a href='/blog/tag/hyperparameter'><li> hyperparameter </li></a>
					
						<a href='/blog/tag/optimization'><li> optimization </li></a>
					
						<a href='/blog/tag/predictive'><li> predictive </li></a>
					
						<a href='/blog/tag/statistics'><li> statistics </li></a>
					
						<a href='/blog/tag/model'><li> model </li></a>
					
						<a href='/blog/tag/iteration'><li> iteration </li></a>
					
						<a href='/blog/tag/automation'><li> automation </li></a>
					
						<a href='/blog/tag/Bayesian'><li> bayesian </li></a>
					
						<a href='/blog/tag/Bayes'><li> bayes </li></a>
					
				</ul>
				<a href='/blog.html'><p>See all posts</p></a>
			</center>
			
			<footer>
				<hr>
				© 2020 Victor Blancada. All Rights Reserved.
			</footer>
			<div class="spacer">
			</div>
		</div>
	</body>
</html>

