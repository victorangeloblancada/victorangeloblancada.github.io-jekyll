---
layout: post
title: "AI Biases and Independent Variables"
featured-img: https://cdn-images-1.medium.com/max/1000/1*2xdYJn8yXgCkDMXytLOujQ.png
date: 2019-01-20
---

Quartz recently published a [thought-provoking article by Twain Liu](https://qz.com/1515889/aristotles-binary-philosophies-created-todays-ai-bias/) that argues that AI bias can be traced all the way back to Aristotle’s philosophy of duality. While the author was correct in acknowledging the existence of human bias and the fact that bias in AI comes from being fed data built by biased human behavior, the argument that machine biases stem from the way machines encode data — in this case, the binary system — is flawed.

The author argued that by reducing real life to ones and zeroes, the binary system misrepresents reality. This argument is misleading as it claims that information is lost when real measurements are converted to binary, which is mathematically not the case. Any whole number in base ten may be converted into binary and vice versa — this is true for any numerical base. Among the numerical bases, binary was chosen as the most efficient way to represent information as it can represent any natural number with the least number of unique symbols (called bits).

However, there is more to the original article’s argument beyond conversion between numerical systems: the author argues that Western philosophy’s focus on duality is what spawned the binary system and that Western philosophy is inherently biased, thus making the binary system itself biased. The argument is that by labeling real-life characteristics with ones and zeroes, the binary system and the concept of duality assign a greater value to one characteristic and a lesser value on its opposite. The example given is Aristotle applying dualism to humans, dividing us into men and women. Like many men from thousands of years ago, Aristotle held very biased views against the fairer sex. The author quotes Aristotle, “The relation of male to female is by nature a relation of superior to inferior and ruler to ruled.”

While the author is correct in saying that people incorrectly assign value based on binary characteristics — going on to rightly list issues of inequality based on ideas such as race and gender — the idea that replacing the binary system will fix these ills is not properly supported. The argument presented in the article is that the binary system not only assigns a greater value on certain characteristics but also endorses spurious correlations across characteristics. This is not a fault of the binary system but an issue with how people interpret data.

Logic and reasoning are fueled by a desire to make good decisions based on available data. The idea is that logical rules can be formulated to predict an outcome by looking for relationships between observations. For instance, dark clouds in the sky signal that rain is coming. In many instances of bias, these incorrect interpretations of data result in false relationships being formed and used as a flawed basis for decision-making. Singing out of tune before a thunderstorm does not mean that bad singing causes bad weather. Bringing the conversation back to the societal biases discussed in the original article: a single terrorist from the Middle East does not mean that everyone from the Middle East is a terrorist.

To correctly use data to arrive at informed decisions, these false relationships must be removed from one’s logical system. The concept of independence among variables is important: while some variables may display a similar trend, this does not immediately imply that one is the cause for another. Correlation does not imply causation, despite anything less logical decision-makers may say. Switching from the binary system to an alternative does not solve the issues with AI bias, but acknowledging the existence of independent variables and building machine learning models that take possible misinterpretations of correlations into account will.

One example used in the original article to prove the argument against binary systems are tree-based machine learning algorithms. For the uninitiated, tree-based algorithms predict a target feature by repeatedly splitting a sample based on other features to arrive at branches that are more and more homogeneous along the target feature. The author of the original article argues that binary incorrectly classifies entities into two groups when real life characteristics behave closer to a scale. However, the very core logic of tree-based models takes this continuous scale into account — I won’t go into the technical details but the focus of the algorithms that underpin tree-based models is on testing splits across the entire continuum until it arrives at a split that maximizes homogeneity among the resulting branches. Furthermore, some algorithms may repeat splits on the same feature, so the original author’s claim of machine learning algorithms pigeonholing entities that are continuous in real life is incorrect as a tree-based model may continually split on the same feature for as long as it improves the model, thus allowing tree-based models to accurately depict non-binary, continuous reality. This proves that the binary nature of systems does not result in a less accurate representation of reality.

However, a machine learning model is only as good as its data. The author of the original article correctly points out the racial and sexual biases that existed during the time of Aristotle and live on even in the age of artificial intelligence. The answer does not lie in replacing the binary system but in being more careful with independent variables. If the binary flags for two features which are proven to be independent are not conflated into a single feature, then the purported bias in the binary system disappears. Yes, it is unfair that men have historically been seen as superior to women, but on the other hand, it is also fair to say that a female athlete is likely to be better at sports than a man who has not exercised in a while because the second argument measures athletic ability without being misled by conflating athleticism with gender.

Funnily enough, this idea of independence between variables is one of the driving ideas behind the Cartesian coordinate system — one of the many contributions of Rene Descartes, to whom the original article lays the blame for the proliferation of bias through the binary system. The Cartesian coordinate system plots measurements across two perpendicular axes so as to visually confirm or debunk any dependent relationships. By definition, measurements along perpendicular axes are independent — one cannot move left by only moving up. Rene Descartes realized the importance of treating variables independently and many novice machine learning practitioners would do well to follow in his example.

While the original article ended with the firm (if unfounded) conclusion that the binary system is at fault for bias in artificial intelligence, this article takes some of the original author’s valid ideas and will not judge the original article as entirely without merit. The truth is somewhere in between — the article managed to highlight key social issues, yes, but the conclusion is not fully valid, no. This judgment only serves to show that the binary system is no less effective in terms of retaining and communicating information than alternatives, by acknowledging that the article can be correct in some points but not all. Until quantum computing allows for qubits to relay and process data more efficiently than bits, the binary system is still the best solution we have today.
